import nltk as nltk
import re
from sentiment_analysis_training_set_generation import file_processor

def capitalized_not_sentence_beginner(text):
    '''(str) -> {str}
    Accepts a raw string and finds all the words in the string that are capitalized
    but which don't begin a sentence. Then removes stopwords, geographic place
    names, and rare entries before returning the remaining words in a set.'''

    ### Use Regex to find all words that begin with a capital letter but that
    ### don't begin a sentence. Note: still captures words at beginning of sentence
    ### that begin their paragraph.
    pattern = re.compile(r'[^.?!-:\'][^\'«[]([A-Z][a-zà-ÿ]+)')
    results = pattern.findall(text)

    ### Remove stopwords, which should be capitalized given we're only working
    ### with capitals
    custom_stopwords = ['oui', 'non', 'merci', 'quand', 'où', 'comme', 'oh',
        'très', 'comment', 'pardon', 'va', 'vont', 'allez', 'allons', 'vas',
        'vais', 'alors', 'bien', 'tout', 'oh', 'ah', 'bon', 'dieu', 'pardon',
        'après', 'pendant','mme','madame']
    stopwords = nltk.corpus.stopwords.words('french') + custom_stopwords
    stopwords = [i.title() for i in stopwords]
    results = [i for i in results if i not in stopwords]

    ### A list of geographic place names and adjectives to remove from results
    fh = open('./data/geographic_stop_words.txt','r')
    geographic_stop_words = fh.read()
    fh.close()
    results = [i for i in results if i not in geographic_stop_words.title()]

    ### Count occurrences of each key, with a goal of removing rare words
    count = {}
    for i in results:
        count.setdefault(i,0)
        count[i] = count[i]+1

    results = {i for i in count.keys() if count[i] > 4}
    return results

def uncapitalized_speakers(text):
    '''(str) -> {str}
    Accepts a raw string and finds instances of a uncapitalized words preceding a colon
    followed by opening guimets, indicating a speaker. Removes rare results
    before returning a set of results.'''

    ### One of the challenges with capitalized names is that honorifics
    ### (e.g. docteur, sergent) and family names following honorifics
    ### (e.g. ispravnik) aren't capitalized.

    ### As the dialogue in the text is semi-structured
    ### we can use that to identify speakers whose names aren't capitalized:
    pattern = re.compile(r'\s([a-zà-ÿ]+): «')
    results = pattern.findall(text)
    ### Count occurrences of each key, with a goal of removing rare words
    count = {}
    for i in results:
        count.setdefault(i,0)
        count[i] = count[i]+1
    results = {i for i in count.keys() if count[i] > 9}
    return results

def execute_char_id(file, beginning_of_text,end_of_text):
    '''(str,str,str) -> {str}
    Accepts a file path, a string indicating the beginning of the text (of interest),
    and a string indicating the end of the text (of interest). Returns a set
    of character names that results from combining the sets generated by
    capitalized_names and uncapitalized_names.'''

    ### Load text file, remove licence
    raw = file_processor(file,beginning_of_text,end_of_text)

    ### Generate set of proper nouns expected to be people
    capitalized_names = capitalized_not_sentence_beginner(raw)
    uncapitalized_names = uncapitalized_speakers(raw)
    characters = capitalized_names|uncapitalized_names

    return characters
